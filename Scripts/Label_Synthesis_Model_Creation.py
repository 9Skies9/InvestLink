# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ykk3LRG1TkKn1NS3nalr249mc7UKkiW7
"""

#!/usr/bin/env python3
"""
train_two_sided_lgbm.py

Pipeline:
- Load user_info.csv (investors) and company_info.csv (companies).
- Compute features for (user, company) pairs:
    * text_similarity
    * industry_overlap
    * stage_fit
    * place_fit
    * check_fit (uses user min/max check vs company fund size)
- Synthetic labels:
    * User side: score_user(features) -> base_label_user in {0,1,None}
    * Company side: score_company(features) -> base_label_company in {0,1,None}
    * For each side, for base labels 0/1:
        - 33% chance -> label = -1 (no interaction)
        - else 15% chance -> flip 0 <-> 1 (noise)
- Write:
    * user_to_company_interact.csv:   u_id,c_id,like_or_not
    * company_to_user_interact.csv:  c_id,u_id,like_or_not
- Train:
    * lgbm_user_model.txt    (P(user likes company))
    * lgbm_company_model.txt (P(company likes user))
"""

import random
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

import pandas as pd
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# -----------------------------
# Config
# -----------------------------
USER_CSV = "user_info.csv"
COMPANY_CSV = "company_info.csv"

USER_TO_COMPANY_OUT = "user_to_company_interact.csv"
COMPANY_TO_USER_OUT = "company_to_user_interact.csv"

NO_INTERACTION_PROB = 0.33   # 33% -> -1, no interaction
FLIP_PROB = 0.15             # 15% flip 0<->1, on remaining labels

SCORE_HI = 0.6               # synthetic threshold -> 1
SCORE_LO = 0.3               # synthetic threshold -> 0

MAX_PAIRS_PER_USER = 200     # cap companies per user to control cost; None for all
RANDOM_SEED = 42
random.seed(RANDOM_SEED)

# -----------------------------
# Data classes
# -----------------------------

@dataclass
class CompanyProfile:
    id: int
    name: str
    desc: str
    industries: List[str]
    stage: str
    place: str
    fund_size: Optional[float]  # numeric (dollars)


@dataclass
class InvestorProfile:
    id: int
    name: str
    desc: str
    industries: List[str]
    stages: List[str]
    places: List[str]
    check_min: Optional[float]  # numeric
    check_max: Optional[float]


# -----------------------------
# TEXT ENCODER (Sentence-BERT)
# -----------------------------

_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
_tokenizer = AutoTokenizer.from_pretrained(_MODEL_NAME)
_model = AutoModel.from_pretrained(_MODEL_NAME)


def _mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return (token_embeddings * input_mask_expanded).sum(1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


def encode_texts(texts: List[str]) -> torch.Tensor:
    enc = _tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        out = _model(**enc)
    emb = _mean_pooling(out, enc["attention_mask"])
    emb = F.normalize(emb, p=2, dim=1)
    return emb  # [N, D]


# -----------------------------
# Helpers: money parsing
# -----------------------------

def parse_money(x: str) -> Optional[float]:
    """
    Parse strings like '$150k', '$3m', '200000' -> float (dollars).
    Return None on failure.
    """
    if x is None:
        return None
    s = str(x).strip()
    if not s:
        return None
    s = s.replace("$", "").lower()
    try:
        if s.endswith("k"):
            return float(s[:-1]) * 1e3
        if s.endswith("m"):
            return float(s[:-1]) * 1e6
        return float(s)
    except Exception:
        return None


# -----------------------------
# Feature helpers
# -----------------------------

def jaccard(a: List[str], b: List[str]) -> float:
    A, B = set(a), set(b)
    if not A and not B:
        return 0.0
    return len(A & B) / float(len(A | B))


def stage_fit(inv_stages: List[str], c_stage: str) -> int:
    return int(c_stage in inv_stages)


def place_fit(inv_places: List[str], c_place: str) -> int:
    return int(c_place in inv_places)


def check_fit(u_min: Optional[float], u_max: Optional[float], company_amount: Optional[float]) -> float:
    """
    Use both min and max check size vs company fund size.
    - If no numbers: return neutral ~0.3
    - If company round << investor_min: low fit
    - Else: use coverage_ratio = company_amount / mid_check
            and turn into 0..1 score.
    """
    if u_min is None or u_max is None or company_amount is None:
        return 0.3

    if u_min <= 0 or u_max <= 0 or company_amount <= 0:
        return 0.3

    u_min, u_max = sorted([u_min, u_max])
    u_mid = 0.5 * (u_min + u_max)

    # company too small for this investor?
    if company_amount < u_min:
        rel = company_amount / u_min  # 0..1
        return max(0.0, min(0.3, rel * 0.3))

    coverage_ratio = company_amount / u_mid  # how many such checks to cover round
    base = 1.0 / (1.0 + coverage_ratio)
    fit = min(1.0, base * 2.0)
    return fit


# -----------------------------
# Load data from CSV
# -----------------------------

def load_investors_from_csv(path: str) -> Dict[int, InvestorProfile]:
    """
    Adjust column names here to match your user_info.csv.
    Assumed columns:
      - 'id'
      - 'U_name'
      - 'U_desc'
      - 'U_industries'        (comma-separated)
      - 'U_stages'            (comma-separated)
      - 'U_places'            (comma-separated)
      - 'U_check size min'
      - 'U_check size max'
    """
    df = pd.read_csv(path)
    investors: Dict[int, InvestorProfile] = {}
    for _, row in df.iterrows():
        uid = int(row["id"])
        industries = [s.strip() for s in str(row.get("U_industries", "")).split(",") if s.strip()]
        stages = [s.strip() for s in str(row.get("U_stages", "")).split(",") if s.strip()]
        places = [s.strip() for s in str(row.get("U_places", "")).split(",") if s.strip()]

        check_min = parse_money(row.get("U_check size min"))
        check_max = parse_money(row.get("U_check size max"))

        investors[uid] = InvestorProfile(
            id=uid,
            name=str(row.get("U_name", uid)),
            desc=str(row.get("U_desc", "")),
            industries=industries,
            stages=stages,
            places=places,
            check_min=check_min,
            check_max=check_max,
        )
    return investors


def load_companies_from_csv(path: str) -> Dict[int, CompanyProfile]:
    """
    Adjust column names here to match your company_info.csv.
    Assumed columns:
      - 'id'
      - 'C_name'
      - 'C_desc'
      - 'C_industries' (comma-separated)
      - 'C_stage'
      - 'C_place'
      - 'C_fund_size'
    """
    df = pd.read_csv(path)
    companies: Dict[int, CompanyProfile] = {}
    for _, row in df.iterrows():
        cid = int(row["id"])
        industries = [s.strip() for s in str(row.get("C_industries", "")).split(",") if s.strip()]
        fund = parse_money(row.get("C_fund_size"))

        companies[cid] = CompanyProfile(
            id=cid,
            name=str(row.get("C_name", cid)),
            desc=str(row.get("C_desc", "")),
            industries=industries,
            stage=str(row.get("C_stage", "")),
            place=str(row.get("C_place", "")),
            fund_size=fund,
        )
    return companies


# -----------------------------
# Synthetic scoring & labels
# -----------------------------

def compute_features_for_pair(
    inv: InvestorProfile,
    comp: CompanyProfile,
    inv_emb: torch.Tensor,
    comp_emb: torch.Tensor,
) -> Dict[str, float]:
    text_sim = float(F.cosine_similarity(inv_emb, comp_emb, dim=0).item())
    ind_ov = jaccard(inv.industries, comp.industries)
    s_fit = stage_fit(inv.stages, comp.stage)
    p_fit = place_fit(inv.places, comp.place)
    c_fit = check_fit(inv.check_min, inv.check_max, comp.fund_size)
    return {
        "text_similarity": text_sim,
        "industry_overlap": ind_ov,
        "stage_fit": float(s_fit),
        "place_fit": float(p_fit),
        "check_fit": c_fit,
    }


def synthetic_scores(features: Dict[str, float]) -> Tuple[float, float]:
    """
    Return (score_user, score_company) with different weightings.
    """
    t = features["text_similarity"]
    ind = features["industry_overlap"]
    sf = features["stage_fit"]
    pf = features["place_fit"]
    cf = features["check_fit"]

    # user-to-company label
    score_user = (
        0.35 * t +
        0.25 * ind +
        0.15 * sf +
        0.10 * pf +
        0.15 * cf
    )

    # company-to-user label
    score_company = (
        0.30 * t +
        0.25 * ind +
        0.15 * sf +
        0.10 * pf +
        0.20 * cf
    )

    return score_user, score_company


def base_label_from_score(score: float, hi: float = SCORE_HI, lo: float = SCORE_LO) -> Optional[int]:
    """
    score >= hi  -> 1
    score <= lo  -> 0
    else         -> None (ambiguous / no label)
    """
    if score >= hi:
        return 1
    if score <= lo:
        return 0
    return None


def apply_thinning_and_noise(label: Optional[int]) -> int:
    """
    Take base label in {0,1,None} and apply:
      - if None -> -1 (no interaction)
      - if 0/1:
          * with NO_INTERACTION_PROB -> -1
          * else with FLIP_PROB      -> flip 0<->1
          * else -> keep
    Returns final label in {-1,0,1}
    """
    if label is None:
        return -1

    # Thinning to no interaction
    if random.random() < NO_INTERACTION_PROB:
        return -1

    # Noise flipping
    if random.random() < FLIP_PROB:
        return 1 - label

    return label


# -----------------------------
# Generate CSVs + build training sets
# -----------------------------

FEATURE_COLS = ["text_similarity", "industry_overlap", "stage_fit", "place_fit", "check_fit"]

def generate_and_build_training(
    investors: Dict[int, InvestorProfile],
    companies: Dict[int, CompanyProfile],
):
    inv_ids = list(investors.keys())
    comp_ids = list(companies.keys())

    # Precompute embeddings
    inv_descs = [investors[i].desc for i in inv_ids]
    comp_descs = [companies[c].desc for c in comp_ids]
    inv_emb_all = encode_texts(inv_descs)
    comp_emb_all = encode_texts(comp_descs)

    inv_emb_map = {inv_ids[i]: inv_emb_all[i] for i in range(len(inv_ids))}
    comp_emb_map = {comp_ids[i]: comp_emb_all[i] for i in range(len(comp_ids))}

    # CSV rows
    user_csv_rows = []
    company_csv_rows = []

    # Training rows
    user_train_rows = []
    company_train_rows = []

    for uid in inv_ids:
        inv = investors[uid]
        inv_emb = inv_emb_map[uid]

        candidates = comp_ids.copy()
        random.shuffle(candidates)
        if MAX_PAIRS_PER_USER is not None:
            candidates = candidates[:MAX_PAIRS_PER_USER]

        for cid in candidates:
            comp = companies[cid]
            comp_emb = comp_emb_map[cid]

            feats = compute_features_for_pair(inv, comp, inv_emb, comp_emb)
            score_u, score_c = synthetic_scores(feats)

            base_u = base_label_from_score(score_u)
            base_c = base_label_from_score(score_c)

            final_u = apply_thinning_and_noise(base_u)
            final_c = apply_thinning_and_noise(base_c)

            # CSV:
            user_csv_rows.append({"u_id": uid, "c_id": cid, "like_or_not": final_u})
            company_csv_rows.append({"c_id": cid, "u_id": uid, "like_or_not": final_c})

            # Training tables (only keep labels 0/1)
            if final_u in (0, 1):
                user_train_rows.append({
                    "investor_id": uid,
                    "company_id": cid,
                    **feats,
                    "label": final_u,
                })
            if final_c in (0, 1):
                company_train_rows.append({
                    "investor_id": uid,
                    "company_id": cid,
                    **feats,
                    "label": final_c,
                })

    # Write CSVs
    pd.DataFrame(user_csv_rows).to_csv(USER_TO_COMPANY_OUT, index=False)
    pd.DataFrame(company_csv_rows).to_csv(COMPANY_TO_USER_OUT, index=False)
    print(f"Wrote {len(user_csv_rows)} rows to {USER_TO_COMPANY_OUT}")
    print(f"Wrote {len(company_csv_rows)} rows to {COMPANY_TO_USER_OUT}")

    user_df = pd.DataFrame(user_train_rows)
    company_df = pd.DataFrame(company_train_rows)

    print("[user] train rows:", len(user_df))
    print(user_df["label"].value_counts())
    print("[company] train rows:", len(company_df))
    print(company_df["label"].value_counts())

    return user_df, company_df


# -----------------------------
# Train LightGBM models
# -----------------------------

def train_lgbm(df: pd.DataFrame, side: str) -> lgb.Booster:
    X = df[FEATURE_COLS]
    y = df["label"]

    X_train, X_valid, y_train, y_valid = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
    )

    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid)

    params = {
        "objective": "binary",
        "metric": "auc",
        "learning_rate": 0.05,
        "num_leaves": 31,
        "verbose": -1,
        # all features are "higher is better"
        "monotone_constraints": [1, 1, 1, 1, 1],
    }

    model = lgb.train(
        params,
        train_set=train_data,
        valid_sets=[valid_data],
        valid_names=[f"{side}_valid"],
        num_boost_round=200,
    )
    return model


# -----------------------------
# Main
# -----------------------------

def main():
    investors = load_investors_from_csv(USER_CSV)
    companies = load_companies_from_csv(COMPANY_CSV)

    if not investors or not companies:
        raise SystemExit("No investors or companies loaded. Check CSVs and column names.")

    user_df, company_df = generate_and_build_training(investors, companies)

    user_model = train_lgbm(user_df, "user")
    company_model = train_lgbm(company_df, "company")

    user_model.save_model("lgbm_user_model.txt")
    company_model.save_model("lgbm_company_model.txt")
    print("Saved models: lgbm_user_model.txt, lgbm_company_model.txt")


if __name__ == "__main__":
    main()